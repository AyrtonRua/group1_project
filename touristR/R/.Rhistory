#accessing Twitter via API login
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <-
'76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#
#
input <- paste("#", keyword, sep = "")
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) magrittr::%>% as.tibble() magrittr::%>% rename("long" = lon) magrittr::%>%
mutate(city = geocode_id) magrittr::%>% select(city, long, lat)
#run query for all data per  month going from past 6 month until today
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input,
n = 10,
lang = "en",
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter) magrittr::%>% tibble::as.tibble()
results_tweetdata$created <- lubridate::ymd_hms(result$created,
tz=Sys.timezone(),
quiet=TRUE) magrittr::%>%
lubridate::date()
#return dataframe results
return(results_tweetdata)
}
track_keyword <- function(keyword) {
#accessing Twitter via API login
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <-
'76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#
#
input <- paste("#", keyword, sep = "")
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) magrittr::%>% as.tibble() magrittr::%>% rename("long" = lon) magrittr::%>%
mutate(city = geocode_id) magrittr::%>% select(city, long, lat)
#run query for all data per  month going from past 6 month until today
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input,
n = 10,
lang = "en",
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter) magrittr::%>% tibble::as.tibble()
results_tweetdata$created <- lubridate::ymd_hms(result$created,
tz=Sys.timezone(),
quiet=TRUE) magrittr::%>%
lubridate::date()
#return dataframe results
return(results_tweetdata)
}
track_keyword <- function(keyword) {
#accessing Twitter via API login
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <-
'76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#
#
input <- paste("#", keyword, sep = "")
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% as.tibble() %>% rename("long" = lon) %>%
mutate(city = geocode_id) %>% select(city, long, lat)
#run query for all data per  month going from past 6 month until today
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input,
n = 10,
lang = "en",
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
results_tweetdata$created <- lubridate::ymd_hms(result$created,
tz=Sys.timezone(),
quiet=TRUE) %>%
lubridate::date()
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
library(roxygen2)
track_keyword <- function(keyword) {
#accessing Twitter via API login
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <-
'76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#
#
input <- paste("#", keyword, sep = "")
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% as.tibble() %>% rename("long" = lon) %>%
mutate(city = geocode_id) %>% select(city, long, lat)
#run query for all data per  month going from past 6 month until today
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input,
n = 10,
lang = "en",
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
results_tweetdata$created <- lubridate::ymd_hms(result$created,
tz=Sys.timezone(),
quiet=TRUE) %>%
lubridate::date()
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
track_keyword <- function(keyword) {
#accessing Twitter via API login
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <-
'76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#
#
input <- paste("#", keyword, sep = "")
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% as.tibble() %>% rename("long" = lon) %>%
mutate(city = geocode_id) %>% select(city, long, lat)
#run query for all data per  month going from past 6 month until today
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input,
n = 10,
lang = "en",
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
results_tweetdata$created <- lubridate::ymd_hms(result$created,
tz=Sys.timezone(),
quiet=TRUE) %>%
lubridate::date()
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
install.packages("sinew")
library(sinew)
usethis::use_package("dplyr")
roxygenize()
library(roxygen2)
roxygenize()
devtools::document()
usethis::use_pipe()
track_keyword <- function(keyword) {
# #accessing Twitter via API login
#   consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
#   consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
#   access_token <-
#     '76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
#   access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
#
#   twitteroauth <- options("httr_oauth_cache")
#   options(httr_oauth_cache=TRUE)
#
#     twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#
#     options(httr_oauth_cache=origop)
#
#   #
#
#
#
#     ## check to see if the token is loaded
#     identical(twitter_token, get_token())
#
library(tidyverse)
input <- paste("#", keyword, sep = "")
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% as.tibble() %>% rename("long" = lon) %>%
mutate(city = geocode_id) %>% select(city, long, lat)
#run query for all data per  month going from past 6 month until today
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input ,
n = 10,
lang = "en"  ,
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
results_tweetdata$created <- lubridate::ymd_hms(result$created,
tz=Sys.timezone(),
quiet=TRUE) %>%
lubridate::date()
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
#example
result <- track_keyword(keyword = "bern")
track_keyword <- function(keyword) {
#accessing Twitter via API from cached filed .httr-oauth
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% tibble::as.tibble() %>% dplyr::rename("long" = lon) %>%
dplyr::mutate(city = keyword) %>% dplyr::select(city, long, lat)
#run query for all data per  month going from past 6 month until today
input <- paste("#", keyword, sep = "")
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input ,
n = 10,
lang = "en"  ,
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
#correct date format and keep it with seconds to have a very specific
#time precision level e.g. to be used during marketing campaigns
#to identify the time slots where users post the most about a given keywork
#e.g. to optimize when to run an ad during the day (when people interact/engage
#the most with our keyword=>more engagement increases the visibility of the company)
#and for a tourist useful to know e.g. what is the current hot spot e.g. coffee shop to go to now
#a resaonable time frame to aggregate results is hours in that sense
results_tweetdata$created <- lubridate::ymd_hms(results_tweetdata$created,
tz=Sys.timezone(),
quiet=TRUE) %>% lubridate::round_date(unit="hour")
#creating the plot of keyword count per day over time (measure of popularity)
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
?get_oauth_sig
??get_oauth_sig
track_keyword <- function(keyword) {
#accessing Twitter via API from cached filed .httr-oauth
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% tibble::as.tibble() %>% dplyr::rename("long" = lon) %>%
dplyr::mutate(city = keyword) %>% dplyr::select(city, long, lat)
#run query for all data per  month going from past 6 month until today
input <- paste("#", keyword, sep = "")
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input ,
n = 10,
lang = "en"  ,
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
#correct date format and keep it with seconds to have a very specific
#time precision level e.g. to be used during marketing campaigns
#to identify the time slots where users post the most about a given keywork
#e.g. to optimize when to run an ad during the day (when people interact/engage
#the most with our keyword=>more engagement increases the visibility of the company)
#and for a tourist useful to know e.g. what is the current hot spot e.g. coffee shop to go to now
#a resaonable time frame to aggregate results is hours in that sense
results_tweetdata$created <- lubridate::ymd_hms(results_tweetdata$created,
tz=Sys.timezone(),
quiet=TRUE) %>% lubridate::round_date(unit="hour")
#creating the plot of keyword count per day over time (measure of popularity)
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
devtools::document()
track_keyword <- function(keyword) {
#accessing Twitter via API
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <- '76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% tibble::as.tibble() %>% dplyr::rename("long" = lon) %>%
dplyr::mutate(city = keyword) %>% dplyr::select(city, long, lat)
#run query for all data per  month going from past 6 month until today
input <- paste("#", keyword, sep = "")
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input ,
n = 10,
lang = "en"  ,
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
#correct date format and keep it with seconds to have a very specific
#time precision level e.g. to be used during marketing campaigns
#to identify the time slots where users post the most about a given keywork
#e.g. to optimize when to run an ad during the day (when people interact/engage
#the most with our keyword=>more engagement increases the visibility of the company)
#and for a tourist useful to know e.g. what is the current hot spot e.g. coffee shop to go to now
#a resaonable time frame to aggregate results is hours in that sense
results_tweetdata$created <- lubridate::ymd_hms(results_tweetdata$created,
tz=Sys.timezone(),
quiet=TRUE) %>% lubridate::round_date(unit="hour")
#creating the plot of keyword count per day over time (measure of popularity)
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
track_keyword <- function(keyword) {
#accessing Twitter via API
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <- '76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% tibble::as.tibble() %>% dplyr::rename("long" = lon) %>%
dplyr::mutate(city = keyword) %>% dplyr::select(city, long, lat)
#run query for all data per  month going from past 6 month until today
input <- paste("#", keyword, sep = "")
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input ,
n = 10,
lang = "en"  ,
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
#correct date format and keep it with seconds to have a very specific
#time precision level e.g. to be used during marketing campaigns
#to identify the time slots where users post the most about a given keywork
#e.g. to optimize when to run an ad during the day (when people interact/engage
#the most with our keyword=>more engagement increases the visibility of the company)
#and for a tourist useful to know e.g. what is the current hot spot e.g. coffee shop to go to now
#a resaonable time frame to aggregate results is hours in that sense
results_tweetdata$created <- lubridate::ymd_hms(results_tweetdata$created,
tz=Sys.timezone(),
quiet=TRUE) %>% lubridate::round_date(unit="hour")
#creating the plot of keyword count per day over time (measure of popularity)
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document("/Users/younesszarhloul/Desktop/courses 2nd year/Autumn semester 2018/Programming Tools in Data Science/FINAL HOMEWORKS/Project/group1_project/touristR")
track_keyword <- function(keyword) {
#accessing Twitter via API
consumer_key <- 'ugjqg8RGNvuTAL1fEiNtw'
consumer_secret <- '4WjuEbP6QLUN2DwDzyTwmdMES6fgnOsS65fWxpT8I'
access_token <- '76887198-JA3xCVO1vvQMqMDiIobWKKGQxYKSB0CV2lI2PZ7GL'
access_secret <- 'IvzlVOC8KkIaMR5s5K4u2IXbxKQv7EcUSvy2bnaru8gKz'
twitteR::setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#obtain latitude and longitude of the input city/keyword
#and make a dataframe from it city lat and long
df <- ggmap::geocode(location =
keyword ,
source = "dsk",
messaging = FALSE) %>% tibble::as.tibble() %>% dplyr::rename("long" = lon) %>%
dplyr::mutate(city = keyword) %>% dplyr::select(city, long, lat)
#run query for all data per  month going from past 6 month until today
input <- paste("#", keyword, sep = "")
#quer Twitter using the keyword input
results_twitter <-   twitteR::searchTwitter(
input ,
n = 10,
lang = "en"  ,
resultType = "mixed",
since = as.character ((lubridate::ymd(Sys.Date())  - 30))   ,
until = as.character (lubridate::ymd(Sys.Date())),
#specifying the geocode to be sure we only obtain e.g. indeed
#the results published from Paris for search query Paris (tweet should
#be made within a radius of 40 miles from Paris maximum)
geocode = paste(df$lat, df$long, "40mi", sep = ",")
)
#saving search query results in a dataframe
results_tweetdata <-  twitteR::twListToDF(results_twitter)%>% tibble::as.tibble()
#correct date format and keep it with seconds to have a very specific
#time precision level e.g. to be used during marketing campaigns
#to identify the time slots where users post the most about a given keywork
#e.g. to optimize when to run an ad during the day (when people interact/engage
#the most with our keyword=>more engagement increases the visibility of the company)
#and for a tourist useful to know e.g. what is the current hot spot e.g. coffee shop to go to now
#a resaonable time frame to aggregate results is hours in that sense
results_tweetdata$created <- lubridate::ymd_hms(results_tweetdata$created,
tz=Sys.timezone(),
quiet=TRUE) %>% lubridate::round_date(unit="hour")
#creating the plot of keyword count per day over time (measure of popularity)
#return dataframe results
return(results_tweetdata)
}
#example
result <- track_keyword(keyword = "bern")
devtools::document()
devtools::document()
